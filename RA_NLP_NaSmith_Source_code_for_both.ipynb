{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYgZgV9pHgRyF0uwZeaZRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ananya-bajaj-DS/Kaggle/blob/main/RA_NLP_NaSmith_Source_code_for_both.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "96S2_BfnDVeD"
      },
      "outputs": [],
      "source": [
        "#--------------------2.1--------------------------------\n",
        "import codecs\n",
        "train_data=[]\n",
        "c = 0\n",
        "with codecs.open('train.txt', 'r', encoding='utf-8', errors='replace') as f:\n",
        "    c += 1\n",
        "\n",
        "    train_data = [line.strip().split(\"\\t\") for line in f]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx_lDBBgLj2J",
        "outputId": "f04db09c-f5b7-4e81-cddb-299f16961962"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khnbmjwvTILV",
        "outputId": "5530abcc-9d14-445a-8a0a-f26d0e2ef96f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En route , they pick up a seemingly-harmless hitchhiker , and continue their journey , only for their car to break down in a deserted motel on a lonely highway .',\n",
              " 'En route , they pick up a seemingly-harmless hitchhiker , and continue their , journey only for their car to break down in a deserted motel on a lonely highway .']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "y_train=[]\n",
        "a=0\n",
        "#1 is correct sequence -> correct, incorrect\n",
        "\n",
        "for i in range(1, len(train_data), 2):\n",
        "    train_data[i][0], train_data[i][1] = train_data[i][1], train_data[i][0]\n",
        "\n",
        "X_train=train_data\n",
        "\n",
        "label=1\n",
        "y_train=[]\n",
        "for i in range(0, len(train_data)):\n",
        "    # train_data[i][0], train_data[i][1] = train_data[i][1], train_data[i][0]\n",
        "    y_train.append(label)\n",
        "    if label==1:\n",
        "        label=0\n",
        "    else:\n",
        "        label=1\n",
        "\n",
        "print(len(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drv8HcHlL4_E",
        "outputId": "30f3181d-bc66-4399-c525-df90ec5e0d88"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))\n",
        "print(len(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH3Mjv0zRMQK",
        "outputId": "6c508f60-6ee6-4eb0-8bcc-639ccd47ab29"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000\n",
            "1000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def extract_features(data):\n",
        "    # concatenate the English and corrupted sentences together, separated by a special token\n",
        "    data_concatenated = [pair[0] + \" <sep> \" + pair[1] for pair in data]\n",
        "    # tokenize the sentences using the NLTK word_tokenize function\n",
        "    data_tokenized = [nltk.word_tokenize(sentence) for sentence in data_concatenated]\n",
        "    # create a vocabulary from the tokenized sentences\n",
        "    vocab = set(word for sentence in data_tokenized for word in sentence)\n",
        "    # create a dictionary that maps each word in the vocabulary to a unique integer index\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    # create a feature matrix by encoding each sentence as a list of integer indices\n",
        "    feature_matrix = [[word_to_idx[word] for word in sentence] for sentence in data_tokenized]\n",
        "\n",
        "    return feature_matrix\n"
      ],
      "metadata": {
        "id": "e3pcdXc4ECS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# import gensim.downloader as api\n",
        "# nltk.download('punkt')\n",
        "# model = api.load(\"glove-wiki-gigaword-300\")\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "# def get_sentence_vector(sentence, model):\n",
        "#     words = nltk.word_tokenize(sentence)\n",
        "#     vector = np.zeros(300)\n",
        "#     for word in words:\n",
        "#         if word in model.vocab:\n",
        "#             vector += model[word]\n",
        "#     return vector\n",
        "\n",
        "# def extract_features(data, model):\n",
        "#     num_samples = len(data)\n",
        "#     feature_matrix = np.zeros((num_samples, 600))\n",
        "#     for i, (correct_sentence, corrupted_sentence) in enumerate(data):\n",
        "#         correct_vector = get_sentence_vector(correct_sentence, model)\n",
        "#         corrupted_vector = get_sentence_vector(corrupted_sentence, model)\n",
        "#         feature_matrix[i, :] = np.concatenate((correct_vector, corrupted_vector))\n",
        "#     return feature_matrix\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-UCc0MSUQRG",
        "outputId": "9a8e1a7c-21ec-41dd-da2c-3c0357dd3472"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = extract_features(X_train, model)\n",
        "print(features)\n",
        "features = extract_features(X_train)"
      ],
      "metadata": {
        "id": "hAwcssFhVfX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(random_state=42, solver='lbfgs', max_iter=6000)\n",
        "clf.fit(features, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Y9efDrs8V0nU",
        "outputId": "77ccfff0-b1ce-46f5-ce29-4c5dcf9fd27f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=6000, random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=6000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=6000, random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train set\n",
        "c=0\n",
        "with codecs.open('test.rand.txt', 'r', encoding='utf-8', errors='replace') as f:\n",
        "    c=c+1\n",
        "    test_data = [line.strip().split(\"\\t\") for line in f]\n",
        "\n",
        "print(test_data[0])    \n",
        "print(len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6Aq_s_fV_j_",
        "outputId": "e9c80d78-13be-45df-923f-8bf4b8628172"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I am absolutely convinced that if this body had existed from the start, we would have managed to be better coordinated, we would have known more, and we would have been more effective.', 'I am absolutely convinced that if this body had existed from the start, we would have managed to be better celebration, we would have known more, and we would have been more effective.']\n",
            "100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_test = extract_features(test_data, model)\n",
        "\n",
        "y_pred = clf.predict(features_test)"
      ],
      "metadata": {
        "id": "q_YDqPStoT38"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a list of test results called 'test_results'\n",
        "# Open the file for writing\n",
        "with open('part1.txt', 'w') as f:\n",
        "    # Loop through each test result\n",
        "    for result in y_pred:\n",
        "        # Write the result to the file\n",
        "        if result==1:\n",
        "            f.write('A' + '\\n')\n",
        "\n",
        "        else:\n",
        "            f.write('B' + '\\n')\n"
      ],
      "metadata": {
        "id": "ToZuJE4Euk2L"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# import random\n",
        "# import os\n",
        "# nltk.download('omw-1.4')\n",
        "\n",
        "# # Check if the wordnet corpus is downloaded; if not, download it\n",
        "# if not os.path.exists(os.path.join(nltk.data.find('corpora'), 'wordnet')):\n",
        "#     nltk.download('wordnet')\n",
        "\n",
        "# from nltk.corpus import wordnet\n",
        "\n",
        "# # Get a list of all the words in the wordnet corpus\n",
        "# all_words = set(wordnet.words())\n",
        "\n",
        "# def get_synonyms(word):\n",
        "#     \"\"\"Get a list of synonyms for a given word\"\"\"\n",
        "#     synonyms = []\n",
        "#     for syn in wordnet.synsets(word):\n",
        "#         for lemma in syn.lemmas():\n",
        "#             synonym = lemma.name()\n",
        "#             if synonym in all_words:  # Only include synonyms that are in the wordnet corpus\n",
        "#                 synonyms.append(synonym)\n",
        "#     return synonyms\n",
        "\n",
        "# def misspell(word):\n",
        "#     \"\"\"Misspell a given word by randomly swapping adjacent characters\"\"\"\n",
        "#     if len(word) <= 1:\n",
        "#         return word\n",
        "#     else:\n",
        "#         i = random.randint(0, len(word) - 2)\n",
        "#         return word[:i] + word[i+1] + word[i] + word[i+2:]\n",
        "\n",
        "# with codecs.open('train.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
        "#     lines = f.readlines()\n",
        "\n",
        "# with open(\"ruined_train.txt\", \"w\") as f:\n",
        "#     for line in lines:\n",
        "#         english_str, corruption_str = line.strip().split(\"\\t\")\n",
        "#         words = english_str.split()\n",
        "#         new_sentence = \"\"\n",
        "#         for word in words:\n",
        "#             # Replace the word with a synonym, or misspell it with a certain probability\n",
        "#             if random.random() < 0.5:\n",
        "#                 synonyms = get_synonyms(word)\n",
        "#                 if synonyms:\n",
        "#                     new_word = synonyms[0]  # Replace the word with the first synonym in the list\n",
        "#                 else:\n",
        "#                     new_word = word\n",
        "#             else:\n",
        "#                 new_word = misspell(word)\n",
        "#             new_sentence += new_word + \" \"  # Add a space after each word\n",
        "#         f.write(f\"{english_str}\\t{new_sentence}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqTn11zww3qi",
        "outputId": "49782544-0f5e-49d1-882a-69aec0c27ab6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------2.2----------------------------\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "import os\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Check if the wordnet corpus is downloaded; if not, download it\n",
        "if not os.path.exists(os.path.join(nltk.data.find('corpora'), 'wordnet')):\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Get a list of all the words in the wordnet corpus\n",
        "all_words = set(wordnet.words())\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"Get a list of synonyms for a given word\"\"\"\n",
        "    synonyms = []\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name()\n",
        "            if synonym in all_words:  # Only include synonyms that are in the wordnet corpus\n",
        "                synonyms.append(synonym)\n",
        "    return synonyms\n",
        "\n",
        "def misspell(word):\n",
        "    \"\"\"Misspell a given word by randomly swapping adjacent characters\"\"\"\n",
        "    if len(word) <= 1:\n",
        "        return word\n",
        "    else:\n",
        "        i = random.randint(0, len(word) - 2)\n",
        "        return word[:i] + word[i+1] + word[i] + word[i+2:]\n",
        "\n",
        "with codecs.open('train.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(\"ruined_train-2.txt\", \"w\") as f:\n",
        "    for line in lines:\n",
        "        english_str, corruption_str = line.strip().split(\"\\t\")\n",
        "        words = english_str.split()\n",
        "        new_sentence = \"\"\n",
        "        for word in words:\n",
        "            # Replace the word with a synonym, or misspell it with a certain probability\n",
        "            if random.random() < 0.5:\n",
        "                synonyms = get_synonyms(word)\n",
        "                if synonyms:\n",
        "                    new_word = synonyms[0]  # Replace the word with the first synonym in the list\n",
        "                else:\n",
        "                    new_word = word\n",
        "            else:\n",
        "                new_word = misspell(word)\n",
        "            new_sentence += new_word + \" \"  # Add a space after each word\n",
        "        f.write(f\"{english_str}\\t{new_sentence}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW9CV0vt8LcG",
        "outputId": "5bfe1103-7aa4-41e7-aa17-38c0a596aceb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-UfdfMvKDTnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}